{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed70a480-3686-4522-acee-ae48079579d2",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">\n",
    "  <a href=\"https://uptrain.ai\">\n",
    "    <img width=\"300\" src=\"https://user-images.githubusercontent.com/108270398/214240695-4f958b76-c993-4ddd-8de6-8668f4d0da84.png\" alt=\"uptrain\">\n",
    "  </a>\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c074abda-7ad0-4af1-a7b4-f5177213dae8",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center;\">Fine-tuning a Large-Language Model</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f34304-dc81-4fd9-94dc-a7dfc371673a",
   "metadata": {},
   "source": [
    "### Install Required packages\n",
    "- [PyTorch](https://pytorch.org/get-started/locally/): Deep learning framework.\n",
    "- Hugging Face Transformers(https://huggingface.co/docs/transformers/installation): To use pretrained state-of-the-art models.\n",
    "- [Hugging Face Datasets](https://pypi.org/project/datasets/): Use public Hugging Face datasets\n",
    "- [IPywidgets](https://ipywidgets.readthedocs.io/en/stable/user_install.html): For interactive notebook widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52468837-a2f1-4531-bef5-b95276db6b82",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/linuz/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages (1.11.0)\n",
      "Requirement already satisfied: transformers in /Users/linuz/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages (4.19.2)\n",
      "Requirement already satisfied: datasets in /Users/linuz/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages (2.9.0)\n",
      "Requirement already satisfied: ipywidgets in /Users/linuz/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages (8.0.4)\n",
      "Collecting uptrain\n",
      "  Downloading uptrain-0.0.3-py3-none-any.whl (48 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.3/48.3 kB\u001b[0m \u001b[31m837.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /Users/linuz/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages (from torch) (4.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/linuz/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /Users/linuz/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages (from transformers) (0.12.0)\n",
      "Requirement already satisfied: filelock in /Users/linuz/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/linuz/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages (from transformers) (1.24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/linuz/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /Users/linuz/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/linuz/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/linuz/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: requests in /Users/linuz/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages (from transformers) (2.28.2)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /Users/linuz/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages (from datasets) (11.0.0)\n",
      "Requirement already satisfied: aiohttp in /Users/linuz/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages (from datasets) (3.8.3)\n",
      "Requirement already satisfied: pandas in /Users/linuz/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages (from datasets) (1.5.3)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /Users/linuz/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages (from datasets) (2023.1.0)\n",
      "Requirement already satisfied: dill<0.3.7 in /Users/linuz/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: multiprocess in /Users/linuz/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: responses<0.19 in /Users/linuz/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: xxhash in /Users/linuz/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages (from datasets) (3.2.0)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /Users/linuz/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages (from ipywidgets) (6.20.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /Users/linuz/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages (from ipywidgets) (8.9.0)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0 in /Users/linuz/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages (from ipywidgets) (4.0.5)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0 in /Users/linuz/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages (from ipywidgets) (3.0.5)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /Users/linuz/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages (from ipywidgets) (5.8.1)\n",
      "Collecting scikit-learn>=0.24.2\n",
      "  Downloading scikit_learn-1.2.1-cp38-cp38-macosx_10_9_x86_64.whl (9.0 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hCollecting plotly>=5.0.0\n",
      "  Downloading plotly-5.13.0-py2.py3-none-any.whl (15.2 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.2/15.2 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting pydantic>=1.9.0\n",
      "  Downloading pydantic-1.10.4-cp38-cp38-macosx_10_9_x86_64.whl (2.9 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /Users/linuz/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages (from aiohttp->datasets) (22.2.0)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /Users/linuz/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages (from aiohttp->datasets) (2.1.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/linuz/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/linuz/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages (from aiohttp->datasets) (1.8.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/linuz/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/linuz/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/linuz/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: debugpy>=1.0 in /Users/linuz/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (1.6.6)\n",
      "Requirement already satisfied: comm>=0.1.1 in /Users/linuz/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.2)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /Users/linuz/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (8.0.1)\n",
      "Requirement already satisfied: appnope in /Users/linuz/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.3)\n",
      "Requirement already satisfied: psutil in /Users/linuz/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (5.9.4)\n",
      "Requirement already satisfied: tornado>=6.1 in /Users/linuz/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (6.2)\n",
      "Requirement already satisfied: pyzmq>=17 in /Users/linuz/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (25.0.0)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /Users/linuz/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: nest-asyncio in /Users/linuz/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (1.5.6)\n",
      "Requirement already satisfied: decorator in /Users/linuz/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.30 in /Users/linuz/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.36)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /Users/linuz/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (2.14.0)\n",
      "Requirement already satisfied: jedi>=0.16 in /Users/linuz/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.18.2)\n",
      "Requirement already satisfied: stack-data in /Users/linuz/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.2)\n",
      "Requirement already satisfied: pexpect>4.3 in /Users/linuz/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: backcall in /Users/linuz/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: pickleshare in /Users/linuz/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/linuz/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages (from pandas->datasets) (2022.7.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/linuz/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages (from pandas->datasets) (2.8.2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tenacity>=6.2.0\n",
      "  Downloading tenacity-8.1.0-py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/linuz/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/linuz/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/linuz/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages (from requests->transformers) (1.26.14)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Using cached threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Collecting joblib>=1.1.1\n",
      "  Using cached joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /Users/linuz/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages (from scikit-learn>=0.24.2->uptrain) (1.10.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /Users/linuz/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /Users/linuz/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (5.1.5)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.3 in /Users/linuz/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (6.0.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/linuz/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /Users/linuz/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages (from prompt-toolkit<3.1.0,>=3.0.30->ipython>=6.1.0->ipywidgets) (0.2.6)\n",
      "Requirement already satisfied: six>=1.5 in /Users/linuz/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in /Users/linuz/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (1.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /Users/linuz/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.2.1)\n",
      "Requirement already satisfied: pure-eval in /Users/linuz/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/linuz/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages (from importlib-metadata>=4.8.3->jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (3.12.0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /Users/linuz/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages (from jupyter-core!=5.0.*,>=4.12->jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (2.6.2)\n",
      "Installing collected packages: threadpoolctl, tenacity, pydantic, joblib, scikit-learn, plotly, uptrain\n",
      "Successfully installed joblib-1.2.0 plotly-5.13.0 pydantic-1.10.4 scikit-learn-1.2.1 tenacity-8.1.0 threadpoolctl-3.1.0 uptrain-0.0.3\n"
     ]
    }
   ],
   "source": [
    "!pip install torch transformers datasets ipywidgets uptrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00207410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-02-06 19:19:08--  https://github.com/uptrain-ai/uptrain/raw/main/examples/4_bert_llm/model_constants.py\n",
      "Resolving github.com (github.com)... 20.207.73.82\n",
      "Connecting to github.com (github.com)|20.207.73.82|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/uptrain-ai/uptrain/main/examples/4_bert_llm/model_constants.py [following]\n",
      "--2023-02-06 19:19:09--  https://raw.githubusercontent.com/uptrain-ai/uptrain/main/examples/4_bert_llm/model_constants.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 185 [text/plain]\n",
      "Saving to: ‘model_constants.py’\n",
      "\n",
      "model_constants.py  100%[===================>]     185  --.-KB/s    in 0s      \n",
      "\n",
      "2023-02-06 19:19:09 (4.52 MB/s) - ‘model_constants.py’ saved [185/185]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/uptrain-ai/uptrain/raw/main/examples/4_bert_llm/model_constants.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9eb81aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-02-06 19:19:11--  https://github.com/uptrain-ai/uptrain/raw/main/examples/4_bert_llm/model_train.py\n",
      "Resolving github.com (github.com)... 20.207.73.82\n",
      "Connecting to github.com (github.com)|20.207.73.82|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/uptrain-ai/uptrain/main/examples/4_bert_llm/model_train.py [following]\n",
      "--2023-02-06 19:19:12--  https://raw.githubusercontent.com/uptrain-ai/uptrain/main/examples/4_bert_llm/model_train.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1819 (1.8K) [text/plain]\n",
      "Saving to: ‘model_train.py’\n",
      "\n",
      "model_train.py      100%[===================>]   1.78K  --.-KB/s    in 0s      \n",
      "\n",
      "2023-02-06 19:19:12 (13.3 MB/s) - ‘model_train.py’ saved [1819/1819]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/uptrain-ai/uptrain/raw/main/examples/4_bert_llm/model_train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78812de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-02-06 19:20:40--  https://github.com/uptrain-ai/uptrain/raw/main/examples/4_bert_llm/helper_funcs.py\n",
      "Resolving github.com (github.com)... 20.207.73.82\n",
      "Connecting to github.com (github.com)|20.207.73.82|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/uptrain-ai/uptrain/main/examples/4_bert_llm/helper_funcs.py [following]\n",
      "--2023-02-06 19:20:41--  https://raw.githubusercontent.com/uptrain-ai/uptrain/main/examples/4_bert_llm/helper_funcs.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4332 (4.2K) [text/plain]\n",
      "Saving to: ‘helper_funcs.py’\n",
      "\n",
      "helper_funcs.py     100%[===================>]   4.23K  --.-KB/s    in 0s      \n",
      "\n",
      "2023-02-06 19:20:42 (9.59 MB/s) - ‘helper_funcs.py’ saved [4332/4332]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/uptrain-ai/uptrain/raw/main/examples/4_bert_llm/helper_funcs.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f61005f8-27ae-4a7a-a190-fe5f335821cb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "from model_constants import *\n",
    "from model_train import retrain_model\n",
    "from helper_funcs import *\n",
    "import json\n",
    "import uptrain\n",
    "import torch\n",
    "# this ensures that the current MacOS version is at least 12.3+\n",
    "print(torch.backends.mps.is_available())\n",
    "# this ensures that the current current PyTorch installation was built with MPS activated.\n",
    "print(torch.backends.mps.is_built())\n",
    "dtype = torch.float\n",
    "device = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d6e1263-e704-4de2-af99-507ea68125fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)\n",
    "testing_text = \"Nike shoes are very [MASK].\"\n",
    "original_model_outputs = test_model(model, testing_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d9edc5da-8366-4c55-93a7-4601c89c9970",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting the folder:  uptrain_smart_data_bert\n"
     ]
    }
   ],
   "source": [
    "def nike_text_present_func(inputs, outputs, gts=None, extra_args={}):\n",
    "    is_present = []\n",
    "    for input in inputs[\"text\"]:\n",
    "        this_present = \"nike\" in input.lower()\n",
    "        is_present.append(bool(this_present))\n",
    "    return is_present\n",
    "\n",
    "uptrain_save_fold_name = \"uptrain_smart_data_bert\"\n",
    "nike_text_present = uptrain.Signal(\"Nike Text Present\", nike_text_present_func)\n",
    "\n",
    "cfg = {\n",
    "    'checks': [{\n",
    "        'type': uptrain.Anomaly.EDGE_CASE,\n",
    "        \"signal_formulae\": nike_text_present\n",
    "    }],\n",
    "\n",
    "    # Define where to save the retraining dataset\n",
    "    'retraining_folder': uptrain_save_fold_name,\n",
    "    \n",
    "    # Define when to retrain, define a large number because we\n",
    "    # are not retraining yet\n",
    "    'retrain_after': 10000000000\n",
    "}\n",
    "\n",
    "framework = uptrain.Framework(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "abe66970-c0ce-421d-9f7a-845453615903",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50  edge cases identified out of  197  total samples\n",
      "100  edge cases identified out of  397  total samples\n",
      "150  edge cases identified out of  597  total samples\n",
      "200  edge cases identified out of  797  total samples\n",
      "250  edge cases identified out of  997  total samples\n",
      "300  edge cases identified out of  1197  total samples\n",
      "350  edge cases identified out of  1397  total samples\n",
      "400  edge cases identified out of  1597  total samples\n",
      "450  edge cases identified out of  1797  total samples\n",
      "500  edge cases identified out of  1997  total samples\n",
      "550  edge cases identified out of  2197  total samples\n",
      "600  edge cases identified out of  2397  total samples\n",
      "650  edge cases identified out of  2597  total samples\n",
      "700  edge cases identified out of  2797  total samples\n",
      "750  edge cases identified out of  2997  total samples\n",
      "800  edge cases identified out of  3197  total samples\n",
      "850  edge cases identified out of  3397  total samples\n",
      "900  edge cases identified out of  3597  total samples\n",
      "950  edge cases identified out of  3797  total samples\n",
      "1000  edge cases identified out of  3997  total samples\n"
     ]
    }
   ],
   "source": [
    "raw_dataset = create_sample_dataset(\"data.json\")\n",
    "with open(raw_dataset) as f:\n",
    "    all_data = json.load(f)\n",
    "\n",
    "for sample in all_data['data']:\n",
    "    inputs = {'data': {'text': [sample['text']]}}\n",
    "    framework.log(inputs = inputs, outputs = None)\n",
    "\n",
    "retraining_dataset = create_dataset_from_csv(uptrain_save_fold_name + \"/1/smart_data.csv\", \"text\", \"retrain_dataset.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ca8d91d9-4540-4850-ad27-69fe9468e7dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-d2d70f9c8f339009\n",
      "Found cached dataset json (/Users/linuz/.cache/huggingface/datasets/json/default-d2d70f9c8f339009/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4eaedcf3ce1849a58f7e3d496b80106d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/linuz/.cache/huggingface/datasets/json/default-d2d70f9c8f339009/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-917a4b96a4c3b86e.arrow\n",
      "Loading cached processed dataset at /Users/linuz/.cache/huggingface/datasets/json/default-d2d70f9c8f339009/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-a1682c8182e25c1e.arrow\n",
      "Loading cached split indices for dataset at /Users/linuz/.cache/huggingface/datasets/json/default-d2d70f9c8f339009/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-431ca2af5112a606.arrow and /Users/linuz/.cache/huggingface/datasets/json/default-d2d70f9c8f339009/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-edd67712f81abd40.arrow\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 00:32]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 93\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>Before training, Perplexity: 2.32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6/6 03:01, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.086400</td>\n",
       "      <td>1.024335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.009500</td>\n",
       "      <td>0.996117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.697200</td>\n",
       "      <td>0.758949</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11\n",
      "  Batch size = 64\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>After training, Perplexity: 2.55\n"
     ]
    }
   ],
   "source": [
    "retrain_model(model, retraining_dataset)\n",
    "retrained_model_outputs = test_model(model, testing_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f014895c-0e6d-4886-a9b4-7071154e6315",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "# Create Nike review training dataset\n",
    "\n",
    "# Download the dataset from the url, zip it and copy the csv file here\n",
    "raw_nike_reviews_dataset = create_dataset_from_csv(\"/Users/linuz/Code/Machine_Learning/flipr/kaggle dataset/web_scraped.csv\", \"Content\", \"raw_nike_reviews_data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c19d809c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['shoes', 'socks', 'jeans', 'sneakers', 'boots']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model(model, 'Nike shoes and [MASK]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcde6e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
